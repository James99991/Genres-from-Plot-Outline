{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model Building\n",
    "\n",
    "Date: 01/12/2019\n",
    "\n",
    "## Loading and Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# Change working directory for locating dataset\n",
    "os.chdir(\"..\")\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Libraries for producing images\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration for saving images\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "FOLDER = \"notebooks\"\n",
    "IMAGE_FOLDER = \"images\"\n",
    "\n",
    "# Dataset location\n",
    "from genres_from_plot_outline.config import dataset_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_dataset(loc):\n",
    "    \"\"\"\n",
    "    IMDb ID's sometimes contain a '0' before the ID, thus \n",
    "    we must ensure we load it as a string and not a number.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = pd.read_csv(loc, usecols = ['imdbId', 'title', 'plot_outline', 'genres'], dtype = {'imdbId': str})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_main_dataset(dataset_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data\n",
    "\n",
    "To avoid snooping bias - will be done later instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_set, test_set = train_test_split(data_full, test_size=0.2, \n",
    "#                                        random_state=42)\n",
    "\n",
    "# print(len(train_set), \"train +\", len(test_set), \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy so as not to harm the original\n",
    "# Don't wan't to keep indexes\n",
    "#data = train_set.copy(deep = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_list_extractor(dataset):\n",
    "    '''\n",
    "    Creates a column containing the genres as a list\n",
    "    '''\n",
    "    data = dataset.copy()\n",
    "    genres_list = []\n",
    "    \n",
    "    for i in data.index:\n",
    "        genre_string = str(data['genres'][i])\n",
    "        genre_list = genre_string.replace(',', '').split(\" \")\n",
    "        genres_list.append(genre_list)\n",
    "        \n",
    "    data['genre_list'] = genres_list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = genre_list_extractor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Rows without any Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_nan_genres(data):\n",
    "    '''\n",
    "    Deletes rows where the genre is listed as nan\n",
    "    '''\n",
    "    rows_with_nan = []\n",
    "    for i in data.index:\n",
    "        if 'nan' in data['genre_list'][i]:\n",
    "            rows_with_nan.append(i)\n",
    "    \n",
    "    data_clean = data.drop(rows_with_nan)\n",
    "    \n",
    "    print(len(data)-len(data_clean), \n",
    "          \"rows with missing genres have been removed\")\n",
    "    \n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 rows with missing genres have been removed\n"
     ]
    }
   ],
   "source": [
    "data = delete_nan_genres(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Rows without Plot Outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rows_with_missing_values(data):\n",
    "    '''\n",
    "    Deletes rows where any column contains a missing value\n",
    "    '''\n",
    "    data_clean = data.dropna()\n",
    "    fraction_removed = (len(data)-len(data_clean))/len(data)\n",
    "    \n",
    "    print(round(fraction_removed*100, 2), \n",
    "          \"percent of rows have been removed\")\n",
    "    \n",
    "    return data_clean    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.37 percent of rows have been removed\n"
     ]
    }
   ],
   "source": [
    "data = delete_rows_with_missing_values(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Number of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_extractor(data):\n",
    "    '''\n",
    "    Creates a column with number of sentences\n",
    "    Don't need specific sentences - too noisy\n",
    "    '''\n",
    "    data = data.copy()\n",
    "    num_sentences_list = []\n",
    "    \n",
    "    for i in data.index:\n",
    "        sentences = sent_tokenize(str(data['plot_outline'][i]).lower())\n",
    "        num_sentences = len(sentences)\n",
    "        num_sentences_list.append(num_sentences)\n",
    "        \n",
    "    data['plot_outline_num_sentences'] = num_sentences_list\n",
    "    \n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sentence_extractor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "# Stemming for feature reduction\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Remove punctuation\n",
    "import string\n",
    "\n",
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(data):\n",
    "    '''\n",
    "    Creates a column with cleaned tokens and the number of tokens\n",
    "    with and without stemming\n",
    "    '''\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stem = SnowballStemmer('english')\n",
    "    \n",
    "    data = data.copy()\n",
    "    \n",
    "    tokens_list = []\n",
    "    tokens_stemmed_list = []\n",
    "    num_tokens_list = []\n",
    "    \n",
    "    for i in data.index:\n",
    "        \n",
    "        # Convert to lowercase and tokenize text\n",
    "        outline = str(data['plot_outline'][i]).lower() \n",
    "        outline_list = wordpunct_tokenize(outline)\n",
    "        \n",
    "        # Using list comprehensions to remove stopwords and punctuation\n",
    "        outline_list_no_stopwords = [word for word in outline_list if not word in stop_words]\n",
    "        tokens = [word for word in outline_list_no_stopwords if not word in list(string.punctuation)]\n",
    "        \n",
    "        # Stemmed tokens \n",
    "        tokens_stem = [stem.stem(word) for word in tokens]\n",
    "        \n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        tokens_list.append(tokens)\n",
    "        tokens_stemmed_list.append(tokens_stem)\n",
    "        num_tokens_list.append(num_tokens)\n",
    "        \n",
    "    data['plot_outline_words'] = tokens_list\n",
    "    data['plot_outline_stemmed_words'] = tokens_stemmed_list\n",
    "    data['plot_outline_num_words'] = num_tokens_list\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdbId</th>\n",
       "      <th>title</th>\n",
       "      <th>plot_outline</th>\n",
       "      <th>genres</th>\n",
       "      <th>genre_list</th>\n",
       "      <th>plot_outline_num_sentences</th>\n",
       "      <th>plot_outline_words</th>\n",
       "      <th>plot_outline_stemmed_words</th>\n",
       "      <th>plot_outline_num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0114709</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>A little boy named Andy loves to be in his roo...</td>\n",
       "      <td>Animation, Adventure, Comedy, Family, Fantasy</td>\n",
       "      <td>[Animation, Adventure, Comedy, Family, Fantasy]</td>\n",
       "      <td>8</td>\n",
       "      <td>[little, boy, named, andy, loves, room, playin...</td>\n",
       "      <td>[littl, boy, name, andi, love, room, play, toy...</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0113497</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>After being trapped in a jungle board game for...</td>\n",
       "      <td>Adventure, Comedy, Family, Fantasy</td>\n",
       "      <td>[Adventure, Comedy, Family, Fantasy]</td>\n",
       "      <td>3</td>\n",
       "      <td>[trapped, jungle, board, game, 26, years, man,...</td>\n",
       "      <td>[trap, jungl, board, game, 26, year, man, chil...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0113228</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>Things don't seem to change much in Wabasha Co...</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>[Comedy, Romance]</td>\n",
       "      <td>3</td>\n",
       "      <td>[things, seem, change, much, wabasha, county, ...</td>\n",
       "      <td>[thing, seem, chang, much, wabasha, counti, ma...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0114885</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>This story based on the best selling novel by ...</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "      <td>3</td>\n",
       "      <td>[story, based, best, selling, novel, terry, mc...</td>\n",
       "      <td>[stori, base, best, sell, novel, terri, mcmill...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0113041</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>In this sequel to \"Father of the Bride\", Georg...</td>\n",
       "      <td>Comedy, Family, Romance</td>\n",
       "      <td>[Comedy, Family, Romance]</td>\n",
       "      <td>3</td>\n",
       "      <td>[sequel, father, bride, \",, george, banks, mus...</td>\n",
       "      <td>[sequel, father, bride, \",, georg, bank, must,...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    imdbId                        title  \\\n",
       "0  0114709                    Toy Story   \n",
       "1  0113497                      Jumanji   \n",
       "2  0113228             Grumpier Old Men   \n",
       "3  0114885            Waiting to Exhale   \n",
       "4  0113041  Father of the Bride Part II   \n",
       "\n",
       "                                        plot_outline  \\\n",
       "0  A little boy named Andy loves to be in his roo...   \n",
       "1  After being trapped in a jungle board game for...   \n",
       "2  Things don't seem to change much in Wabasha Co...   \n",
       "3  This story based on the best selling novel by ...   \n",
       "4  In this sequel to \"Father of the Bride\", Georg...   \n",
       "\n",
       "                                          genres  \\\n",
       "0  Animation, Adventure, Comedy, Family, Fantasy   \n",
       "1             Adventure, Comedy, Family, Fantasy   \n",
       "2                                Comedy, Romance   \n",
       "3                         Comedy, Drama, Romance   \n",
       "4                        Comedy, Family, Romance   \n",
       "\n",
       "                                        genre_list  \\\n",
       "0  [Animation, Adventure, Comedy, Family, Fantasy]   \n",
       "1             [Adventure, Comedy, Family, Fantasy]   \n",
       "2                                [Comedy, Romance]   \n",
       "3                         [Comedy, Drama, Romance]   \n",
       "4                        [Comedy, Family, Romance]   \n",
       "\n",
       "   plot_outline_num_sentences  \\\n",
       "0                           8   \n",
       "1                           3   \n",
       "2                           3   \n",
       "3                           3   \n",
       "4                           3   \n",
       "\n",
       "                                  plot_outline_words  \\\n",
       "0  [little, boy, named, andy, loves, room, playin...   \n",
       "1  [trapped, jungle, board, game, 26, years, man,...   \n",
       "2  [things, seem, change, much, wabasha, county, ...   \n",
       "3  [story, based, best, selling, novel, terry, mc...   \n",
       "4  [sequel, father, bride, \",, george, banks, mus...   \n",
       "\n",
       "                          plot_outline_stemmed_words  plot_outline_num_words  \n",
       "0  [littl, boy, name, andi, love, room, play, toy...                      75  \n",
       "1  [trap, jungl, board, game, 26, year, man, chil...                      22  \n",
       "2  [thing, seem, chang, much, wabasha, counti, ma...                      63  \n",
       "3  [stori, base, best, sell, novel, terri, mcmill...                      39  \n",
       "4  [sequel, father, bride, \",, georg, bank, must,...                      46  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_per_sentence(data):\n",
    "    '''\n",
    "    Function to get number of words (after cleaning) per\n",
    "    sentence\n",
    "    '''\n",
    "    data['words_per_sentence'] = data['plot_outline_num_words']/data['plot_outline_num_sentences']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = words_per_sentence(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecotrising is typically applied to strings of text, \n",
    "# but since the tokenization is already done\n",
    "# we will join the lists of strings together to be later used\n",
    "def cleaned_plot_outline(data):\n",
    "    \"\"\"\n",
    "    Function to join cleaned tokenized words into \n",
    "    full string again\n",
    "    \"\"\"\n",
    "    \n",
    "    data = data.copy()\n",
    "    \n",
    "    cleaned_outlines = []\n",
    "    cleaned_outlines_stemmed = []\n",
    "    \n",
    "    for i in data.index:\n",
    "        \n",
    "        outline_clean = ' '.join(data['plot_outline_words'][i])\n",
    "        outline_clean_stemmed = ' '.join(data['plot_outline_stemmed_words'][i])\n",
    "        \n",
    "        cleaned_outlines.append(outline_clean)\n",
    "        cleaned_outlines_stemmed.append(outline_clean_stemmed)\n",
    "        \n",
    "    data['plot_outline_clean'] = cleaned_outlines\n",
    "    data['plot_outline_clean_stemmed'] = cleaned_outlines_stemmed \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdbId</th>\n",
       "      <th>title</th>\n",
       "      <th>plot_outline</th>\n",
       "      <th>genres</th>\n",
       "      <th>genre_list</th>\n",
       "      <th>plot_outline_num_sentences</th>\n",
       "      <th>plot_outline_words</th>\n",
       "      <th>plot_outline_stemmed_words</th>\n",
       "      <th>plot_outline_num_words</th>\n",
       "      <th>words_per_sentence</th>\n",
       "      <th>plot_outline_clean</th>\n",
       "      <th>plot_outline_clean_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0114709</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>A little boy named Andy loves to be in his roo...</td>\n",
       "      <td>Animation, Adventure, Comedy, Family, Fantasy</td>\n",
       "      <td>[Animation, Adventure, Comedy, Family, Fantasy]</td>\n",
       "      <td>8</td>\n",
       "      <td>[little, boy, named, andy, loves, room, playin...</td>\n",
       "      <td>[littl, boy, name, andi, love, room, play, toy...</td>\n",
       "      <td>75</td>\n",
       "      <td>9.375000</td>\n",
       "      <td>little boy named andy loves room playing toys ...</td>\n",
       "      <td>littl boy name andi love room play toy especi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0113497</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>After being trapped in a jungle board game for...</td>\n",
       "      <td>Adventure, Comedy, Family, Fantasy</td>\n",
       "      <td>[Adventure, Comedy, Family, Fantasy]</td>\n",
       "      <td>3</td>\n",
       "      <td>[trapped, jungle, board, game, 26, years, man,...</td>\n",
       "      <td>[trap, jungl, board, game, 26, year, man, chil...</td>\n",
       "      <td>22</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>trapped jungle board game 26 years man child w...</td>\n",
       "      <td>trap jungl board game 26 year man child win re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0113228</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>Things don't seem to change much in Wabasha Co...</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>[Comedy, Romance]</td>\n",
       "      <td>3</td>\n",
       "      <td>[things, seem, change, much, wabasha, county, ...</td>\n",
       "      <td>[thing, seem, chang, much, wabasha, counti, ma...</td>\n",
       "      <td>63</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>things seem change much wabasha county max joh...</td>\n",
       "      <td>thing seem chang much wabasha counti max john ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0114885</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>This story based on the best selling novel by ...</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "      <td>3</td>\n",
       "      <td>[story, based, best, selling, novel, terry, mc...</td>\n",
       "      <td>[stori, base, best, sell, novel, terri, mcmill...</td>\n",
       "      <td>39</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>story based best selling novel terry mcmillan ...</td>\n",
       "      <td>stori base best sell novel terri mcmillan foll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0113041</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>In this sequel to \"Father of the Bride\", Georg...</td>\n",
       "      <td>Comedy, Family, Romance</td>\n",
       "      <td>[Comedy, Family, Romance]</td>\n",
       "      <td>3</td>\n",
       "      <td>[sequel, father, bride, \",, george, banks, mus...</td>\n",
       "      <td>[sequel, father, bride, \",, georg, bank, must,...</td>\n",
       "      <td>46</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>sequel father bride \", george banks must accep...</td>\n",
       "      <td>sequel father bride \", georg bank must accept ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    imdbId                        title  \\\n",
       "0  0114709                    Toy Story   \n",
       "1  0113497                      Jumanji   \n",
       "2  0113228             Grumpier Old Men   \n",
       "3  0114885            Waiting to Exhale   \n",
       "4  0113041  Father of the Bride Part II   \n",
       "\n",
       "                                        plot_outline  \\\n",
       "0  A little boy named Andy loves to be in his roo...   \n",
       "1  After being trapped in a jungle board game for...   \n",
       "2  Things don't seem to change much in Wabasha Co...   \n",
       "3  This story based on the best selling novel by ...   \n",
       "4  In this sequel to \"Father of the Bride\", Georg...   \n",
       "\n",
       "                                          genres  \\\n",
       "0  Animation, Adventure, Comedy, Family, Fantasy   \n",
       "1             Adventure, Comedy, Family, Fantasy   \n",
       "2                                Comedy, Romance   \n",
       "3                         Comedy, Drama, Romance   \n",
       "4                        Comedy, Family, Romance   \n",
       "\n",
       "                                        genre_list  \\\n",
       "0  [Animation, Adventure, Comedy, Family, Fantasy]   \n",
       "1             [Adventure, Comedy, Family, Fantasy]   \n",
       "2                                [Comedy, Romance]   \n",
       "3                         [Comedy, Drama, Romance]   \n",
       "4                        [Comedy, Family, Romance]   \n",
       "\n",
       "   plot_outline_num_sentences  \\\n",
       "0                           8   \n",
       "1                           3   \n",
       "2                           3   \n",
       "3                           3   \n",
       "4                           3   \n",
       "\n",
       "                                  plot_outline_words  \\\n",
       "0  [little, boy, named, andy, loves, room, playin...   \n",
       "1  [trapped, jungle, board, game, 26, years, man,...   \n",
       "2  [things, seem, change, much, wabasha, county, ...   \n",
       "3  [story, based, best, selling, novel, terry, mc...   \n",
       "4  [sequel, father, bride, \",, george, banks, mus...   \n",
       "\n",
       "                          plot_outline_stemmed_words  plot_outline_num_words  \\\n",
       "0  [littl, boy, name, andi, love, room, play, toy...                      75   \n",
       "1  [trap, jungl, board, game, 26, year, man, chil...                      22   \n",
       "2  [thing, seem, chang, much, wabasha, counti, ma...                      63   \n",
       "3  [stori, base, best, sell, novel, terri, mcmill...                      39   \n",
       "4  [sequel, father, bride, \",, georg, bank, must,...                      46   \n",
       "\n",
       "   words_per_sentence                                 plot_outline_clean  \\\n",
       "0            9.375000  little boy named andy loves room playing toys ...   \n",
       "1            7.333333  trapped jungle board game 26 years man child w...   \n",
       "2           21.000000  things seem change much wabasha county max joh...   \n",
       "3           13.000000  story based best selling novel terry mcmillan ...   \n",
       "4           15.333333  sequel father bride \", george banks must accep...   \n",
       "\n",
       "                          plot_outline_clean_stemmed  \n",
       "0  littl boy name andi love room play toy especi ...  \n",
       "1  trap jungl board game 26 year man child win re...  \n",
       "2  thing seem chang much wabasha counti max john ...  \n",
       "3  stori base best sell novel terri mcmillan foll...  \n",
       "4  sequel father bride \", georg bank must accept ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = cleaned_plot_outline(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from target variable\n",
    "df_x = data[['title', 'plot_outline', 'plot_outline_num_sentences', \n",
    "             'plot_outline_words', 'plot_outline_stemmed_words',\n",
    "            'plot_outline_num_words', 'words_per_sentence', 'plot_outline_clean',\n",
    "            'plot_outline_clean_stemmed']]\n",
    "df_y = data[['genre_list']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After all data exploration, let’s concentrate now on building the actual model. As it is a multi-label classification, we need to convert our target label into a binarised vector with multiple bits set as 1.\n",
    "‘MultiLabelBinarizer’ of ‘scikit-learn’ can do that\n",
    "\n",
    "https://medium.com/towards-artificial-intelligence/multi-label-text-classification-using-scikit-multilearn-case-study-with-stackoverflow-questions-768cb487ad12\n",
    "\n",
    "Since we want to ensure all genres are encorporated, we will apply MultiLabelBinarizer to the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Algorithms\n",
    "\n",
    "### Example 1\n",
    "\n",
    "https://www.kaggle.com/roccoli/multi-label-classification-with-sklearn\n",
    "\n",
    "OneVsRest with different classifiers\n",
    "use OneVsRest strategy to have one classifier for each class/label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "y_encoded = multilabel_binarizer.fit_transform(df_y['genre_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(df_x['plot_outline_clean'])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model training pipeline stuff here now\n",
    "\n",
    "Best try and use gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# just plot outline - no other features\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_tfidf, y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/roccoli/multi-label-classification-with-sklearn\n",
    "\n",
    "# For people reading in 2019, now you can use sklearn.metrics.hamming_loss:\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    http://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/float(len(set_true.union(set_pred)) )\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)\n",
    "\n",
    "def print_score(y_pred, clf):\n",
    "    print(\"Clf: \", clf.__class__.__name__)\n",
    "    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, test_y)))\n",
    "    print(\"Hamming score: {}\".format(hamming_score(y_pred, test_y)))\n",
    "    print(\"---\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf:  MultinomialNB\n",
      "Hamming loss: 0.08981512480005951\n",
      "Hamming score: 0.24793181467938746\n",
      "---\n",
      "Clf:  SGDClassifier\n",
      "Hamming loss: 0.09387593150566033\n",
      "Hamming score: 0.2503737093542918\n",
      "---\n",
      "Clf:  LogisticRegression\n",
      "Hamming loss: 0.07603318082059295\n",
      "Hamming score: 0.36297733299323526\n",
      "---\n",
      "Clf:  MultinomialNB\n",
      "Hamming loss: 0.08981512480005951\n",
      "Hamming score: 0.24793181467938746\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=6, tol=None)\n",
    "lr = LogisticRegression()\n",
    "mn = MultinomialNB()\n",
    "\n",
    "for classifier in [nb_clf, sgd, lr, mn]:\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(train_x, train_y)\n",
    "    y_pred = clf.predict(test_x)\n",
    "    print_score(y_pred, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from target variable\n",
    "df_x = data[['title', 'plot_outline', 'plot_outline_num_sentences', \n",
    "             'plot_outline_words', 'plot_outline_stemmed_words',\n",
    "            'plot_outline_num_words', 'words_per_sentence', 'plot_outline_clean',\n",
    "            'plot_outline_clean_stemmed']]\n",
    "df_y = data[['genre_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "y_encoded = multilabel_binarizer.fit_transform(df_y['genre_list'])\n",
    "y = pd.DataFrame(y_encoded, columns=multilabel_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training and validation set\n",
    "train_x, test_x, train_y, test_y = train_test_split(df_x['plot_outline_clean'], y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Instantiate the vectorizer\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    #stop_words='english',\n",
    "    #sublinear_tf=True,\n",
    "    #strip_accents='unicode',\n",
    "    #analyzer='word',\n",
    "    #token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n",
    "    #ngram_range=(1, 1),\n",
    "    ##max_features=30000\n",
    ")\n",
    "\n",
    "# fit and transform on it the training features\n",
    "word_vectorizer.fit(train_x)\n",
    "train_x_word_features = word_vectorizer.transform(train_x)\n",
    "\n",
    "#transform the test features to sparse matrix\n",
    "# Dont fit to test - just transform!!\n",
    "test_features = word_vectorizer.transform(test_x)\n",
    "\n",
    "# transform the holdout text for submission at the end\n",
    "# holdout_text = holdout['comment_text']\n",
    "# holdout_word_features = word_vectorizer.transform(holdout_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify**\n",
    "\n",
    "Run a Logistic regression on each label separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Log_loss score for class Action is -0.28310804593243993\n",
      "CV Accuracy score for class Action is 0.8828611610809493\n",
      "CV ROC_AUC score 0.886959743146456\n",
      "\n",
      "[[3915  134]\n",
      " [ 393  337]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.97      0.94      4049\n",
      "          1       0.72      0.46      0.56       730\n",
      "\n",
      "avg / total       0.88      0.89      0.88      4779\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Log_loss score for class Adult is -0.0023963046799023243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy score for class Adult is 0.9997384116325859\n",
      "CV ROC_AUC score 0.9497697781498535\n",
      "\n",
      "[[4778    0]\n",
      " [   1    0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      4778\n",
      "          1       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4779\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Log_loss score for class Adventure is -0.2572911639922875\n",
      "CV Accuracy score for class Adventure is 0.8999689422842518\n",
      "CV ROC_AUC score 0.8716391182909599\n",
      "\n",
      "[[4111  120]\n",
      " [ 359  189]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.97      0.94      4231\n",
      "          1       0.61      0.34      0.44       548\n",
      "\n",
      "avg / total       0.88      0.90      0.89      4779\n",
      "\n",
      "CV Log_loss score for class Animation is -0.11513584259079881\n",
      "CV Accuracy score for class Animation is 0.961703590609635\n",
      "CV ROC_AUC score 0.9116233340616124\n",
      "\n",
      "[[4565   12]\n",
      " [ 171   31]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      4577\n",
      "          1       0.72      0.15      0.25       202\n",
      "\n",
      "avg / total       0.95      0.96      0.95      4779\n",
      "\n",
      "CV Log_loss score for class Biography is -0.16121956735678583\n",
      "CV Accuracy score for class Biography is 0.9474208202768419\n",
      "CV ROC_AUC score 0.8621735706713582\n",
      "\n",
      "[[4486   16]\n",
      " [ 247   30]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      4502\n",
      "          1       0.65      0.11      0.19       277\n",
      "\n",
      "avg / total       0.93      0.94      0.93      4779\n",
      "\n",
      "CV Log_loss score for class Comedy is -0.5141997813611515\n",
      "CV Accuracy score for class Comedy is 0.7584497590474877\n",
      "CV ROC_AUC score 0.8131943215250103\n",
      "\n",
      "[[2730  421]\n",
      " [ 721  907]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.87      0.83      3151\n",
      "          1       0.68      0.56      0.61      1628\n",
      "\n",
      "avg / total       0.75      0.76      0.75      4779\n",
      "\n",
      "CV Log_loss score for class Crime is -0.3014796145291713\n",
      "CV Accuracy score for class Crime is 0.8800357608586001\n",
      "CV ROC_AUC score 0.870250827298451\n",
      "\n",
      "[[3916  144]\n",
      " [ 396  323]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.96      0.94      4060\n",
      "          1       0.69      0.45      0.54       719\n",
      "\n",
      "avg / total       0.88      0.89      0.88      4779\n",
      "\n",
      "CV Log_loss score for class Documentary is -0.09750970076646313\n",
      "CV Accuracy score for class Documentary is 0.9594537893629855\n",
      "CV ROC_AUC score 0.976518705032029\n",
      "\n",
      "[[4425   26]\n",
      " [ 163  165]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98      4451\n",
      "          1       0.86      0.50      0.64       328\n",
      "\n",
      "avg / total       0.96      0.96      0.96      4779\n",
      "\n",
      "CV Log_loss score for class Drama is -0.5892563687182468\n",
      "CV Accuracy score for class Drama is 0.7042474311900493\n",
      "CV ROC_AUC score 0.7873909734928222\n",
      "\n",
      "[[1382  711]\n",
      " [ 602 2084]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.66      0.68      2093\n",
      "          1       0.75      0.78      0.76      2686\n",
      "\n",
      "avg / total       0.72      0.73      0.72      4779\n",
      "\n",
      "CV Log_loss score for class Family is -0.1685939274380676\n",
      "CV Accuracy score for class Family is 0.940671755532934\n",
      "CV ROC_AUC score 0.8920521271862729\n",
      "\n",
      "[[4415   27]\n",
      " [ 251   86]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.99      0.97      4442\n",
      "          1       0.76      0.26      0.38       337\n",
      "\n",
      "avg / total       0.93      0.94      0.93      4779\n",
      "\n",
      "CV Log_loss score for class Fantasy is -0.20454875915920603\n",
      "CV Accuracy score for class Fantasy is 0.929266178631092\n",
      "CV ROC_AUC score 0.8443691935708165\n",
      "\n",
      "[[4321   50]\n",
      " [ 314   94]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.99      0.96      4371\n",
      "          1       0.65      0.23      0.34       408\n",
      "\n",
      "avg / total       0.91      0.92      0.91      4779\n",
      "\n",
      "CV Log_loss score for class Film-Noir is -0.061331007924318594\n",
      "CV Accuracy score for class Film-Noir is 0.9855080095195019\n",
      "CV ROC_AUC score 0.896967131090755\n",
      "\n",
      "[[4724    0]\n",
      " [  55    0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      4724\n",
      "          1       0.00      0.00      0.00        55\n",
      "\n",
      "avg / total       0.98      0.99      0.98      4779\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Log_loss score for class History is -0.14533650257688682\n",
      "CV Accuracy score for class History is 0.9521292740118643\n",
      "CV ROC_AUC score 0.8944772228320528\n",
      "\n",
      "[[4536   19]\n",
      " [ 193   31]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98      4555\n",
      "          1       0.62      0.14      0.23       224\n",
      "\n",
      "avg / total       0.94      0.96      0.94      4779\n",
      "\n",
      "CV Log_loss score for class Horror is -0.1866458912693896\n",
      "CV Accuracy score for class Horror is 0.9281154591456897\n",
      "CV ROC_AUC score 0.9235865382019409\n",
      "\n",
      "[[4186   51]\n",
      " [ 291  251]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96      4237\n",
      "          1       0.83      0.46      0.59       542\n",
      "\n",
      "avg / total       0.92      0.93      0.92      4779\n",
      "\n",
      "CV Log_loss score for class Music is -0.10602490304593226\n",
      "CV Accuracy score for class Music is 0.966830700860738\n",
      "CV ROC_AUC score 0.8934720924061799\n",
      "\n",
      "[[4572   25]\n",
      " [ 130   52]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98      4597\n",
      "          1       0.68      0.29      0.40       182\n",
      "\n",
      "avg / total       0.96      0.97      0.96      4779\n",
      "\n",
      "CV Log_loss score for class Musical is -0.12015934883025373\n",
      "CV Accuracy score for class Musical is 0.966307318822583\n",
      "CV ROC_AUC score 0.8663760350976445\n",
      "\n",
      "[[4615    8]\n",
      " [ 137   19]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98      4623\n",
      "          1       0.70      0.12      0.21       156\n",
      "\n",
      "avg / total       0.96      0.97      0.96      4779\n",
      "\n",
      "CV Log_loss score for class Mystery is -0.23257749112511\n",
      "CV Accuracy score for class Mystery is 0.9174951438955026\n",
      "CV ROC_AUC score 0.8311957774026739\n",
      "\n",
      "[[4280   70]\n",
      " [ 316  113]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.98      0.96      4350\n",
      "          1       0.62      0.26      0.37       429\n",
      "\n",
      "avg / total       0.90      0.92      0.90      4779\n",
      "\n",
      "CV Log_loss score for class News is -0.015895193200368078\n",
      "CV Accuracy score for class News is 0.9972271851272122\n",
      "CV ROC_AUC score 0.9761478035387432\n",
      "\n",
      "[[4768    0]\n",
      " [  11    0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      4768\n",
      "          1       0.00      0.00      0.00        11\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4779\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-669dc5646d9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sag'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mcv_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x_word_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_log_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CV Log_loss score for class {} is {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             return_times=True)\n\u001b[1;32m--> 206\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1245\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[0;32m   1246\u001b[0m                              \u001b[1;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1247\u001b[1;33m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[0;32m   1248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "losses = []\n",
    "auc = []\n",
    "\n",
    "for class_name in class_names:\n",
    "    #call the labels one column at a time so we can run the classifier on them\n",
    "    train_target = train_y[class_name]\n",
    "    test_target = test_y[class_name]\n",
    "    classifier = LogisticRegression(solver='sag', C=10)\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, train_x_word_features, train_target, cv=5, scoring='neg_log_loss'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV Log_loss score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(classifier, train_x_word_features, train_target, cv=5, scoring='accuracy'))\n",
    "    print('CV Accuracy score for class {} is {}'.format(class_name, cv_score))\n",
    "    \n",
    "    classifier.fit(train_x_word_features, train_target)\n",
    "    y_pred = classifier.predict(test_features)\n",
    "    y_pred_prob = classifier.predict_proba(test_features)[:, 1]\n",
    "    auc_score = metrics.roc_auc_score(test_target, y_pred_prob)\n",
    "    auc.append(auc_score)\n",
    "    print(\"CV ROC_AUC score {}\\n\".format(auc_score))\n",
    "    \n",
    "    print(confusion_matrix(test_target, y_pred))\n",
    "    print(classification_report(test_target, y_pred))\n",
    "\n",
    "print('Total average CV Log_loss score is {}'.format(np.mean(losses)))\n",
    "print('Total average CV ROC_AUC score is {}'.format(np.mean(auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 - Use ML Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from target variable\n",
    "df_x = data[['title', 'plot_outline', 'plot_outline_num_sentences', \n",
    "             'plot_outline_words', 'plot_outline_stemmed_words',\n",
    "            'plot_outline_num_words', 'words_per_sentence', 'plot_outline_clean',\n",
    "            'plot_outline_clean_stemmed']]\n",
    "df_y = data[['genre_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and shuffle for cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import numpy as np\n",
    "\n",
    "# split all features\n",
    "train_x, test_x, train_y, test_y = train_test_split(df_x, df_y, test_size = 0.2)\n",
    "\n",
    "# Shuffle training set\n",
    "#shuffle_index = np.random.permutation(60000)\n",
    "#train_x, y_train = train_x[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_parser(data, stemmed = False):\n",
    "    \"\"\"\n",
    "    Function to select and split certain attributes\n",
    "    \"\"\"\n",
    "    data_outline_attributes = data[['plot_outline_num_sentences', \n",
    "                                  'plot_outline_num_words',\n",
    "                                  'words_per_sentence']]\n",
    "    if stemmed:\n",
    "        data_outline = data['plot_outline_clean_stemmed']\n",
    "    else:\n",
    "        data_outline = data['plot_outline_clean']\n",
    "        \n",
    "    return data_outline_attributes, data_outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_attributes, df_x_outline = variable_parser(df_x)\n",
    "\n",
    "train_x_attributes, train_x_outline = variable_parser(train_x)\n",
    "test_x_attributes, test_x_outline = variable_parser(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform target variable\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Fit on whole data set - so that genres aren't missing\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(df_y['genre_list'])\n",
    "\n",
    "# transform on both training and test\n",
    "train_y_encoded = multilabel_binarizer.transform(train_y['genre_list'])\n",
    "test_y_encoded = multilabel_binarizer.transform(test_y['genre_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the tfidf on the whole dataset may introduce bias towards the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate the vectorizer, use default for now\n",
    "word_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit and transform on it the training features\n",
    "word_vectorizer.fit(df_x_outline)\n",
    "train_x_word_features = word_vectorizer.transform(train_x_outline)\n",
    "\n",
    "#transform the test features to sparse matrix\n",
    "test_x_word_features = word_vectorizer.transform(test_x_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Want hamming_loss in cross validation**\n",
    "\n",
    "[Here](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values) lists the scoring options. But there's also a section *Defining your scoring strategy from metric functions*\n",
    "\n",
    "```python\n",
    ">>> from sklearn.metrics import fbeta_score, make_scorer\n",
    ">>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    ">>> from sklearn.model_selection import GridSearchCV\n",
    ">>> from sklearn.svm import LinearSVC\n",
    ">>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n",
    "...                     scoring=ftwo_scorer, cv=5)\n",
    "```\n",
    "\n",
    "This issue was raised [here](https://stackoverflow.com/questions/44081222/hamming-loss-not-support-in-cross-val-score)\n",
    "\n",
    ">@iromise In case of losses, scores just return the negative of actual score. That is the intended behaviour, when we want to compare two scores. Because in cases of accuracy or f1_score, the higher score is better, but in case of losses, lower score is better. To handle them both in same way, it returns the negative. Read the section 3.3.1.1 carefully. Just multiply by -1 to return your actual hamming loss values. – Vivek Kumar May 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OneVsRestClassifier classifier here\n",
    "- Could I try OneVsOne here either\n",
    "\n",
    "[On the other hand, LinearSVC implements “one-vs-the-rest” multi-class strategy, thus training n_class models. If there are only two classes, only one model is trained:](https://scikit-learn.org/stable/modules/svm.html)\n",
    "\n",
    "After a promissing model is chosen, we will tune paramaters in tfidf and for stemmed/not stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_and_record_model(clf, train_x_word_features, train_x_attributes, \n",
    "                           train_y_encoded, include_text_attributes = False):\n",
    "    \"\"\"\n",
    "    Function to train model and record results.\n",
    "    No parameter tuning for tfidf and not using stemmed outline\n",
    "    \"\"\"\n",
    "    \n",
    "    # Empty list to store info\n",
    "    model_info = []\n",
    "    \n",
    "    #---- Record name of model -----#\n",
    "    \n",
    "    print(\"Classifier: \", clf.__class__.__name__)\n",
    "    model_info.append(str(clf.__class__.__name__))\n",
    "    print(\"---\") \n",
    "    \n",
    "    #----- Include or not include additional text_attributes -----#\n",
    "    \n",
    "    if include_text_attributes:\n",
    "        \n",
    "        # Concatenating with sparse TFIDF matrix that is pre fit already\n",
    "        train_x_final = hstack((train_x_word_features, train_x_attributes))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        train_x_final = train_x_word_features\n",
    "    \n",
    "    #----- Training classifier -----#\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    start_time = time.time()\n",
    "    clf = OneVsRestClassifier(clf)\n",
    "    \n",
    "    # Getting scores using cross validation with hamming_loss as scorer\n",
    "    cv_scores = (-1) * cross_val_score(clf, train_x_final, train_y_encoded, cv = 5, \n",
    "                                       scoring = make_scorer(hamming_loss, greater_is_better = False))\n",
    "    cv_scores = np.array(cv_scores)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(\"Training time: \", round(training_time/60, 2), \"minutes.\")\n",
    "    model_info.append(round(training_time/60, 2))\n",
    "    \n",
    "    #---- Recording CV results -----#\n",
    "    \n",
    "    hamming_loss_avg_on_val = np.mean(cv_scores)\n",
    "    hamming_loss_std_dev_on_val = np.std(cv_scores)\n",
    "    \n",
    "    model_info.append(hamming_loss_avg_on_val)\n",
    "    model_info.append(hamming_loss_std_dev_on_val)\n",
    "    \n",
    "    print(\"Average Hamming Loss: \", hamming_loss_avg_on_val)\n",
    "    print(\"----------------------\")   \n",
    "    \n",
    "    return pd.Series(model_info, index = ['classifier', 'training_time_mins',\n",
    "                                    'hamming_loss_avg_on_val', \n",
    "                                    'hamming_loss_std_dev_on_val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping Over Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loop_and_record_model_combinations(df_x, train_x, train_y_encoded,\n",
    "                                       vectorizer_list, classifier_list):\n",
    "    \"\"\"\n",
    "    Function to take combinations of,\n",
    "    \n",
    "    parameters and apply models to each one\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    stemmed_record = []\n",
    "    vectorizer_record = []\n",
    "    text_attributes_used_record = []\n",
    "    \n",
    "    all_model_info = pd.DataFrame(columns=['classifier', 'training_time_mins',\n",
    "                                    'hamming_loss_avg_on_val', \n",
    "                                    'hamming_loss_std_dev_on_val'])\n",
    "\n",
    "    #----- Loop over stemmed/not stemmed outlines -----#\n",
    "    for stemmed in [False, True]:\n",
    "\n",
    "        print(\"Stemmed: \", stemmed)\n",
    "        df_x_attributes, df_x_outline = variable_parser(df_x, stemmed = stemmed)\n",
    "        train_x_attributes, train_x_outline = variable_parser(train_x, stemmed = stemmed)\n",
    "\n",
    "        #---- Loop over vectorizer -----#\n",
    "        for vectorizer in vectorizer_list:\n",
    "\n",
    "            print(\"Vectorizer: \", vectorizer, \". Fitting...\")\n",
    "            # Fit on whole dataset but transform on training set\n",
    "            vectorizer.fit(df_x_outline)\n",
    "            train_x_word_features = vectorizer.transform(train_x_outline)\n",
    "\n",
    "            #---- Loop over classifiers -----#\n",
    "            for classifier in classifier_list:\n",
    "\n",
    "                for include_text_attributes in [False, True]:\n",
    "\n",
    "                    print(\"Including text attributes: \", include_text_attributes)\n",
    "                    print(\"---\")\n",
    "                    model_info = apply_and_record_model(classifier, train_x_word_features, \n",
    "                                                        train_x_attributes, train_y_encoded,\n",
    "                                                        include_text_attributes = include_text_attributes)\n",
    "                    all_model_info = all_model_info.append(model_info, ignore_index = True)\n",
    "\n",
    "                    stemmed_record.append(str(stemmed))\n",
    "                    vectorizer_record.append(vectorizer)\n",
    "                    text_attributes_used_record.append(str(include_text_attributes))\n",
    "\n",
    "\n",
    "    all_model_info['vectorizer_record'] = vectorizer_record\n",
    "    all_model_info['stemmed_record'] = stemmed_record\n",
    "    all_model_info['text_attributes_used_record'] = text_attributes_used_record\n",
    "    \n",
    "    run_time = time.time() - start_time\n",
    "    print(\"Run time: \", round(run_time/60, 2), \"minutes.\")\n",
    "    \n",
    "    return all_model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import make_scorer, hamming_loss\n",
    "\n",
    "# Use default parameters\n",
    "nb = MultinomialNB()\n",
    "sgd = SGDClassifier(random_state=42)\n",
    "lr = LogisticRegression(random_state=42)\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "svc = LinearSVC(random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "word_vectorizer = TfidfVectorizer()\n",
    "word_vectorizer_uni_bi = TfidfVectorizer(ngram_range = (1, 2))\n",
    "word_vectorizer_bi = TfidfVectorizer(ngram_range = (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed:  False\n",
      "Vectorizer:  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) . Fitting...\n",
      "Including text attributes:  False\n",
      "---\n",
      "Classifier:  MultinomialNB\n",
      "---\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 18 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.14 minutes.\n",
      "Average Hamming Loss:  0.09034275936705813\n",
      "----------------------\n",
      "Including text attributes:  True\n",
      "---\n",
      "Classifier:  MultinomialNB\n",
      "---\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 18 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.15 minutes.\n",
      "Average Hamming Loss:  0.09299668206953914\n",
      "----------------------\n",
      "Including text attributes:  False\n",
      "---\n",
      "Classifier:  SGDClassifier\n",
      "---\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 18 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.28 minutes.\n",
      "Average Hamming Loss:  0.07355947459398215\n",
      "----------------------\n",
      "Including text attributes:  True\n",
      "---\n",
      "Classifier:  SGDClassifier\n",
      "---\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 18 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.34 minutes.\n",
      "Average Hamming Loss:  0.1580337697757414\n",
      "----------------------\n",
      "Including text attributes:  False\n",
      "---\n",
      "Classifier:  LogisticRegression\n",
      "---\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 18 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  1.49 minutes.\n",
      "Average Hamming Loss:  0.07794752698017682\n",
      "----------------------\n",
      "Including text attributes:  True\n",
      "---\n",
      "Classifier:  LogisticRegression\n",
      "---\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 18 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  6.17 minutes.\n",
      "Average Hamming Loss:  0.07790825376243933\n",
      "----------------------\n",
      "Including text attributes:  False\n",
      "---\n",
      "Classifier:  LinearSVC\n",
      "---\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 18 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\User\\Anaconda3\\envs\\text_analytics\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 23 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  1.77 minutes.\n",
      "Average Hamming Loss:  0.07245986372209597\n",
      "----------------------\n",
      "Including text attributes:  True\n",
      "---\n",
      "Classifier:  LinearSVC\n",
      "---\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "vectorizer_list = [word_vectorizer, word_vectorizer_uni_bi, word_vectorizer_bi]  \n",
    "classifier_list = [nb, sgd, lr, svc]\n",
    "\n",
    "model_and_combination_df = loop_and_record_model_combinations(df_x, train_x, train_y_encoded, \n",
    "                                                              vectorizer_list, classifier_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_and_combination_df.to_csv(r\"C:\\Users\\User\\Documents\\ITB Year 2\\Text Analytics and Web Content Mining\\Assignments\\Assignment 2\\versioned_code\\Genres-from-Plot-Outline\\data\\model_and_combination.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show first\n",
      "show second\n"
     ]
    }
   ],
   "source": [
    "for x in [False, True]:\n",
    "    if x:\n",
    "        print(\"show second\")\n",
    "    else:\n",
    "        print(\"show first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>training_time_mins</th>\n",
       "      <th>hamming_loss_avg_on_val</th>\n",
       "      <th>hamming_loss_std_dev_on_val</th>\n",
       "      <th>vectorizer_record</th>\n",
       "      <th>stemmed_record</th>\n",
       "      <th>text_attributes_used_record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.092838</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.094553</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.080031</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.154750</td>\n",
       "      <td>0.025076</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>11.27</td>\n",
       "      <td>0.085056</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>44.52</td>\n",
       "      <td>0.085103</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.074798</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>139.35</td>\n",
       "      <td>0.097151</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.092838</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.094553</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.080031</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.154750</td>\n",
       "      <td>0.025076</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>9.31</td>\n",
       "      <td>0.085056</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>35.14</td>\n",
       "      <td>0.085103</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.074798</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>124.74</td>\n",
       "      <td>0.097151</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.092838</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.094553</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.080031</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.154750</td>\n",
       "      <td>0.025076</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>7.97</td>\n",
       "      <td>0.085056</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>29.98</td>\n",
       "      <td>0.085103</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.074798</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>123.76</td>\n",
       "      <td>0.097151</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.092838</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.094553</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.080031</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.154750</td>\n",
       "      <td>0.025076</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>7.91</td>\n",
       "      <td>0.085056</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>29.81</td>\n",
       "      <td>0.085103</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.074798</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>124.31</td>\n",
       "      <td>0.097151</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.092838</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.094553</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.080031</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.154750</td>\n",
       "      <td>0.025076</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>7.95</td>\n",
       "      <td>0.085056</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>31.09</td>\n",
       "      <td>0.085103</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.074798</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>124.45</td>\n",
       "      <td>0.097151</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.092838</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.094553</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.080031</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.154750</td>\n",
       "      <td>0.025076</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>7.99</td>\n",
       "      <td>0.085056</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>29.88</td>\n",
       "      <td>0.085103</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.074798</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>124.41</td>\n",
       "      <td>0.097151</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            classifier  training_time_mins  hamming_loss_avg_on_val  \\\n",
       "0        MultinomialNB                0.93                 0.092838   \n",
       "1        MultinomialNB                1.15                 0.094553   \n",
       "2        SGDClassifier                1.11                 0.080031   \n",
       "3        SGDClassifier                1.10                 0.154750   \n",
       "4   LogisticRegression               11.27                 0.085056   \n",
       "5   LogisticRegression               44.52                 0.085103   \n",
       "6            LinearSVC                3.51                 0.074798   \n",
       "7            LinearSVC              139.35                 0.097151   \n",
       "8        MultinomialNB                0.80                 0.092838   \n",
       "9        MultinomialNB                0.82                 0.094553   \n",
       "10       SGDClassifier                0.90                 0.080031   \n",
       "11       SGDClassifier                0.96                 0.154750   \n",
       "12  LogisticRegression                9.31                 0.085056   \n",
       "13  LogisticRegression               35.14                 0.085103   \n",
       "14           LinearSVC                3.53                 0.074798   \n",
       "15           LinearSVC              124.74                 0.097151   \n",
       "16       MultinomialNB                0.72                 0.092838   \n",
       "17       MultinomialNB                0.73                 0.094553   \n",
       "18       SGDClassifier                0.84                 0.080031   \n",
       "19       SGDClassifier                0.89                 0.154750   \n",
       "20  LogisticRegression                7.97                 0.085056   \n",
       "21  LogisticRegression               29.98                 0.085103   \n",
       "22           LinearSVC                3.53                 0.074798   \n",
       "23           LinearSVC              123.76                 0.097151   \n",
       "24       MultinomialNB                0.71                 0.092838   \n",
       "25       MultinomialNB                0.74                 0.094553   \n",
       "26       SGDClassifier                0.84                 0.080031   \n",
       "27       SGDClassifier                0.89                 0.154750   \n",
       "28  LogisticRegression                7.91                 0.085056   \n",
       "29  LogisticRegression               29.81                 0.085103   \n",
       "30           LinearSVC                3.54                 0.074798   \n",
       "31           LinearSVC              124.31                 0.097151   \n",
       "32       MultinomialNB                0.70                 0.092838   \n",
       "33       MultinomialNB                0.73                 0.094553   \n",
       "34       SGDClassifier                0.84                 0.080031   \n",
       "35       SGDClassifier                0.89                 0.154750   \n",
       "36  LogisticRegression                7.95                 0.085056   \n",
       "37  LogisticRegression               31.09                 0.085103   \n",
       "38           LinearSVC                3.66                 0.074798   \n",
       "39           LinearSVC              124.45                 0.097151   \n",
       "40       MultinomialNB                0.72                 0.092838   \n",
       "41       MultinomialNB                0.76                 0.094553   \n",
       "42       SGDClassifier                0.86                 0.080031   \n",
       "43       SGDClassifier                0.89                 0.154750   \n",
       "44  LogisticRegression                7.99                 0.085056   \n",
       "45  LogisticRegression               29.88                 0.085103   \n",
       "46           LinearSVC                3.60                 0.074798   \n",
       "47           LinearSVC              124.41                 0.097151   \n",
       "\n",
       "    hamming_loss_std_dev_on_val  \\\n",
       "0                      0.000709   \n",
       "1                      0.000673   \n",
       "2                      0.000740   \n",
       "3                      0.025076   \n",
       "4                      0.000706   \n",
       "5                      0.000744   \n",
       "6                      0.000977   \n",
       "7                      0.005942   \n",
       "8                      0.000709   \n",
       "9                      0.000673   \n",
       "10                     0.000740   \n",
       "11                     0.025076   \n",
       "12                     0.000706   \n",
       "13                     0.000744   \n",
       "14                     0.000977   \n",
       "15                     0.005942   \n",
       "16                     0.000709   \n",
       "17                     0.000673   \n",
       "18                     0.000740   \n",
       "19                     0.025076   \n",
       "20                     0.000706   \n",
       "21                     0.000744   \n",
       "22                     0.000977   \n",
       "23                     0.005942   \n",
       "24                     0.000709   \n",
       "25                     0.000673   \n",
       "26                     0.000740   \n",
       "27                     0.025076   \n",
       "28                     0.000706   \n",
       "29                     0.000744   \n",
       "30                     0.000977   \n",
       "31                     0.005942   \n",
       "32                     0.000709   \n",
       "33                     0.000673   \n",
       "34                     0.000740   \n",
       "35                     0.025076   \n",
       "36                     0.000706   \n",
       "37                     0.000744   \n",
       "38                     0.000977   \n",
       "39                     0.005942   \n",
       "40                     0.000709   \n",
       "41                     0.000673   \n",
       "42                     0.000740   \n",
       "43                     0.025076   \n",
       "44                     0.000706   \n",
       "45                     0.000744   \n",
       "46                     0.000977   \n",
       "47                     0.005942   \n",
       "\n",
       "                                    vectorizer_record stemmed_record  \\\n",
       "0   TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "1   TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "2   TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "3   TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "4   TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "5   TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "6   TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "7   TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "8   TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "9   TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "10  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "11  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "12  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "13  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "14  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "15  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "16  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "17  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "18  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "19  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "20  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "21  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "22  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "23  TfidfVectorizer(analyzer='word', binary=False,...          False   \n",
       "24  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "25  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "26  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "27  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "28  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "29  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "30  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "31  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "32  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "33  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "34  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "35  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "36  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "37  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "38  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "39  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "40  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "41  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "42  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "43  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "44  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "45  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "46  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "47  TfidfVectorizer(analyzer='word', binary=False,...           True   \n",
       "\n",
       "   text_attributes_used_record  \n",
       "0                        False  \n",
       "1                         True  \n",
       "2                        False  \n",
       "3                         True  \n",
       "4                        False  \n",
       "5                         True  \n",
       "6                        False  \n",
       "7                         True  \n",
       "8                        False  \n",
       "9                         True  \n",
       "10                       False  \n",
       "11                        True  \n",
       "12                       False  \n",
       "13                        True  \n",
       "14                       False  \n",
       "15                        True  \n",
       "16                       False  \n",
       "17                        True  \n",
       "18                       False  \n",
       "19                        True  \n",
       "20                       False  \n",
       "21                        True  \n",
       "22                       False  \n",
       "23                        True  \n",
       "24                       False  \n",
       "25                        True  \n",
       "26                       False  \n",
       "27                        True  \n",
       "28                       False  \n",
       "29                        True  \n",
       "30                       False  \n",
       "31                        True  \n",
       "32                       False  \n",
       "33                        True  \n",
       "34                       False  \n",
       "35                        True  \n",
       "36                       False  \n",
       "37                        True  \n",
       "38                       False  \n",
       "39                        True  \n",
       "40                       False  \n",
       "41                        True  \n",
       "42                       False  \n",
       "43                        True  \n",
       "44                       False  \n",
       "45                        True  \n",
       "46                       False  \n",
       "47                        True  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_and_combination_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
